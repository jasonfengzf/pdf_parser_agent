import os
import requests
import json
from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility
import hashlib
import time
import uuid
from typing import List, Dict

from config import QWEN_API_KEY


class MilvusPaperStore:
    def __init__(self, host='localhost', port='19530', qwen_api_key=None, qwen_base_url=None):
        # è¿æ¥ Milvus
        connections.connect(host=host, port=port)

        # Qwen API é…ç½®
        self.qwen_api_key = qwen_api_key or os.getenv('QWEN_API_KEY')
        self.qwen_base_url = qwen_base_url or os.getenv('QWEN_BASE_URL',
                                                        'https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings')

        # å®šä¹‰é›†åˆ schema - å¢åŠ å®‰å…¨è¾¹ç•Œ
        self.fields = [
            FieldSchema(name="id", dtype=DataType.VARCHAR, is_primary=True, max_length=128),
            FieldSchema(name="file_name", dtype=DataType.VARCHAR, max_length=255),
            FieldSchema(name="file_type", dtype=DataType.VARCHAR, max_length=10),
            FieldSchema(name="directory", dtype=DataType.VARCHAR, max_length=255),
            FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=65535),  # æœ€å¤§ 65535 å­—ç¬¦
            FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=1536)
        ]

        self.schema = CollectionSchema(self.fields, "Academic paper collection")
        self.collection_name = "papers"

        # åˆ›å»ºé›†åˆ
        if utility.has_collection(self.collection_name):
            utility.drop_collection(self.collection_name)

        self.collection = Collection(self.collection_name, self.schema)

        # åˆ›å»ºç´¢å¼•
        index_params = {
            "index_type": "IVF_FLAT",
            "metric_type": "L2",
            "params": {"nlist": 128}
        }
        self.collection.create_index("vector", index_params)
        print(f"é›†åˆ {self.collection_name} åˆ›å»ºæˆåŠŸ")

    def get_target_files(self, extracted_path: str = './output_extracted') -> List[Dict]:
        """è·å–éœ€è¦å…¥åº“çš„æ–‡ä»¶åˆ—è¡¨"""
        if not os.path.exists(extracted_path):
            print(f"ç›®å½•ä¸å­˜åœ¨: {extracted_path}")
            return []

        target_files = []

        print("å¼€å§‹æ‰«æç›®å½•ç»“æ„...")
        for root, dirs, files in os.walk(extracted_path):
            if 'images' in dirs:
                dirs.remove('images')

            for file in files:
                if file == 'full.md' or (file.endswith('content_list.json') and not file.endswith('_origin.pdf')):
                    file_path = os.path.join(root, file)
                    directory = os.path.basename(root)

                    if file == 'full.md':
                        storage_file_name = f"{directory}_full.md"
                    else:
                        storage_file_name = f"{directory}_{file}"

                    file_info = {
                        'file_path': file_path,
                        'original_name': file,
                        'storage_name': storage_file_name,
                        'directory': directory,
                        'file_type': 'md' if file.endswith('.md') else 'json',
                        'relative_path': os.path.relpath(file_path, extracted_path)
                    }
                    target_files.append(file_info)

        return target_files

    def display_file_list(self, file_list: List[Dict]):
        """æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨ä¿¡æ¯"""
        if not file_list:
            print("æœªæ‰¾åˆ°ç›®æ ‡æ–‡ä»¶")
            return

        print(f"\n{'=' * 80}")
        print(f"æ‰¾åˆ° {len(file_list)} ä¸ªéœ€è¦å…¥åº“çš„æ–‡ä»¶:")
        print(f"{'=' * 80}")

        dir_groups = {}
        for file_info in file_list:
            dir_name = file_info['directory']
            if dir_name not in dir_groups:
                dir_groups[dir_name] = []
            dir_groups[dir_name].append(file_info)

        for dir_name, files in dir_groups.items():
            print(f"\nğŸ“ ç›®å½•: {dir_name}")
            for i, file_info in enumerate(files, 1):
                file_size = os.path.getsize(file_info['file_path']) if os.path.exists(file_info['file_path']) else 0
                file_size_mb = file_size / (1024 * 1024)
                print(
                    f"  {i}. {file_info['original_name']} â†’ {file_info['storage_name']} ({file_info['file_type']}) - {file_size_mb:.2f} MB")

    def get_embedding_with_qwen(self, text):
        """ä½¿ç”¨ Qwen API è·å–æ–‡æœ¬å‘é‡"""
        headers = {
            'Authorization': f'Bearer {self.qwen_api_key}',
            'Content-Type': 'application/json'
        }

        # å¦‚æœæ–‡æœ¬å¤ªé•¿ï¼Œè¿›è¡Œæˆªæ–­ï¼ˆQwen é™åˆ¶å¤§çº¦ 2000 å­—ç¬¦ï¼‰
        if len(text) > 2000:
            text = text[:2000]

        data = {
            "model": "text-embedding-v1",
            "input": text
        }

        try:
            response = requests.post(self.qwen_base_url, headers=headers, json=data, timeout=30)
            response.raise_for_status()
            result = response.json()

            if 'data' in result and len(result['data']) > 0:
                return result['data'][0]['embedding']
            elif 'output' in result and 'embeddings' in result['output']:
                return result['output']['embeddings'][0]['embedding']
            else:
                print(f"API å“åº”æ ¼å¼å¼‚å¸¸: {result}")
                import random
                return [random.uniform(-1, 1) for _ in range(1536)]

        except requests.exceptions.HTTPError as e:
            print(f"HTTP é”™è¯¯: {e}")
            if hasattr(e, 'response') and e.response is not None:
                print(f"å“åº”å†…å®¹: {e.response.text}")
            import random
            return [random.uniform(-1, 1) for _ in range(1536)]
        except Exception as e:
            print(f"è·å–å‘é‡å¤±è´¥: {e}")
            import random
            return [random.uniform(-1, 1) for _ in range(1536)]

    def split_content(self, content, max_length=60000):
        """
        å°†é•¿å†…å®¹åˆ†å‰²ä¸ºå¤šä¸ªç‰‡æ®µï¼Œç¡®ä¿æ¯ä¸ªç‰‡æ®µä¸è¶…è¿‡æœ€å¤§é•¿åº¦
        ä½¿ç”¨æ›´ä¿å®ˆçš„åˆ†å‰²ç­–ç•¥ï¼Œç•™å‡ºå®‰å…¨è¾¹ç•Œ
        """
        # è®¾ç½®å®‰å…¨è¾¹ç•Œï¼Œç¡®ä¿ä¸è¶…è¿‡ 65535
        safe_max_length = min(max_length, 65000)

        if len(content) <= safe_max_length:
            return [content]

        chunks = []
        start = 0

        while start < len(content):
            # è®¡ç®—å½“å‰ç‰‡æ®µçš„ç»“æŸä½ç½®ï¼ˆè€ƒè™‘å®‰å…¨è¾¹ç•Œï¼‰
            end = start + safe_max_length

            # å¦‚æœå‰©ä½™å†…å®¹ä¸è¶³ safe_max_lengthï¼Œç›´æ¥å–å‰©ä½™éƒ¨åˆ†
            if end >= len(content):
                chunks.append(content[start:])
                break

            # ä¼˜å…ˆåœ¨æ®µè½è¾¹ç•Œåˆ†å‰²
            paragraph_break = content.rfind('\n\n', start, end)
            if paragraph_break != -1 and paragraph_break > start + safe_max_length * 0.3:  # è‡³å°‘ä¿ç•™30%å†…å®¹
                end = paragraph_break
            else:
                # å…¶æ¬¡åœ¨å¥å­è¾¹ç•Œåˆ†å‰²
                sentence_break = content.rfind('ã€‚', start, end)
                if sentence_break != -1 and sentence_break > start + safe_max_length * 0.3:
                    end = sentence_break + 1  # åŒ…æ‹¬å¥å·
                else:
                    # æœ€ååœ¨æ¢è¡Œç¬¦å¤„åˆ†å‰²
                    line_break = content.rfind('\n', start, end)
                    if line_break != -1 and line_break > start + safe_max_length * 0.3:
                        end = line_break
                    else:
                        # å¼ºåˆ¶åœ¨å®‰å…¨ä½ç½®åˆ†å‰²
                        end = start + safe_max_length

            chunk = content[start:end].strip()
            if chunk:  # ç¡®ä¿ç‰‡æ®µä¸ä¸ºç©º
                chunks.append(chunk)

            start = end

            # è·³è¿‡åˆ†å‰²ç¬¦
            while start < len(content) and content[start] in ['\n', ' ', '\t']:
                start += 1

        return chunks

    def validate_content_length(self, content_chunks):
        """éªŒè¯æ‰€æœ‰å†…å®¹ç‰‡æ®µéƒ½ä¸è¶…è¿‡é•¿åº¦é™åˆ¶"""
        validated_chunks = []
        for i, chunk in enumerate(content_chunks):
            if len(chunk) > 65535:
                print(f"è­¦å‘Š: ç‰‡æ®µ {i + 1} ä»ç„¶è¿‡é•¿ ({len(chunk)} å­—ç¬¦)ï¼Œè¿›è¡Œå¼ºåˆ¶æˆªæ–­")
                # å¼ºåˆ¶æˆªæ–­åˆ°å®‰å…¨é•¿åº¦
                chunk = chunk[:65000]
            validated_chunks.append(chunk)
        return validated_chunks

    def generate_entity_id(self, file_name, content_hash, chunk_index, total_chunks):
        """ç”Ÿæˆç¬¦åˆé•¿åº¦é™åˆ¶çš„å®ä½“ ID"""
        base_id = str(uuid.uuid4())
        if total_chunks > 1:
            return f"{base_id}_{chunk_index}"
        else:
            return base_id

    def store_single_file(self, file_info: Dict):
        """å­˜å‚¨å•ä¸ªæ–‡ä»¶"""
        file_path = file_info['file_path']
        storage_name = file_info['storage_name']
        original_name = file_info['original_name']
        directory = file_info['directory']
        file_type = file_info['file_type']

        try:
            # è¯»å–æ–‡ä»¶å†…å®¹
            if file_type == 'md':
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
            elif file_type == 'json':
                with open(file_path, 'r', encoding='utf-8') as f:
                    json_content = json.load(f)
                content = json.dumps(json_content, ensure_ascii=False, indent=2)
            else:
                print(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path}")
                return False

            # ç”Ÿæˆå†…å®¹å“ˆå¸Œ
            content_hash = hashlib.md5(content.encode()).hexdigest()[:8]

            # æ£€æŸ¥å†…å®¹é•¿åº¦ï¼Œå¦‚æœè¶…è¿‡é™åˆ¶åˆ™åˆ†å‰²
            max_content_length = 65535
            if len(content) > max_content_length:
                print(f"å†…å®¹è¿‡é•¿ ({len(content)} å­—ç¬¦)ï¼Œè¿›è¡Œåˆ†å‰²...")
                content_chunks = self.split_content(content, 60000)  # ä½¿ç”¨æ›´ä¿å®ˆçš„åˆå§‹åˆ†å‰²é•¿åº¦
                content_chunks = self.validate_content_length(content_chunks)  # éªŒè¯é•¿åº¦
                print(f"åˆ†å‰²ä¸º {len(content_chunks)} ä¸ªç‰‡æ®µ")
            else:
                content_chunks = [content]

            # å­˜å‚¨æ¯ä¸ªç‰‡æ®µ
            success_count = 0
            for i, chunk in enumerate(content_chunks):
                # æœ€ç»ˆé•¿åº¦æ£€æŸ¥
                if len(chunk) > max_content_length:
                    print(f"é”™è¯¯: ç‰‡æ®µ {i + 1} ä»ç„¶è¿‡é•¿ ({len(chunk)} å­—ç¬¦)ï¼Œè·³è¿‡æ­¤ç‰‡æ®µ")
                    continue

                # ç”Ÿæˆå”¯ä¸€ ID
                entity_id = self.generate_entity_id(storage_name, content_hash, i, len(content_chunks))

                # è·å–å‘é‡
                print(f"æ­£åœ¨ä¸º {storage_name} ç‰‡æ®µ {i + 1}/{len(content_chunks)} ç”Ÿæˆå‘é‡...")
                vector = self.get_embedding_with_qwen(chunk[:2000])

                # å‡†å¤‡æ•°æ®
                chunk_file_name = storage_name + (f"_part{i + 1}" if len(content_chunks) > 1 else "")

                data = [
                    [entity_id],
                    [chunk_file_name],
                    [file_type],
                    [directory],
                    [chunk],
                    [vector]
                ]

                # æ’å…¥æ•°æ®
                try:
                    self.collection.insert(data)
                    success_count += 1
                    print(f"æ–‡ä»¶ç‰‡æ®µ {i + 1}/{len(content_chunks)} å­˜å‚¨æˆåŠŸ (é•¿åº¦: {len(chunk)} å­—ç¬¦)")
                except Exception as insert_error:
                    print(f"æ’å…¥æ•°æ®å¤±è´¥: {insert_error}")
                    # å°è¯•ä½¿ç”¨æ›´çŸ­çš„ ID
                    short_entity_id = str(uuid.uuid4())[:32]
                    data[0] = [short_entity_id]
                    try:
                        self.collection.insert(data)
                        success_count += 1
                        print(f"æ–‡ä»¶ç‰‡æ®µ {i + 1}/{len(content_chunks)} ä½¿ç”¨çŸ­IDå­˜å‚¨æˆåŠŸ")
                    except Exception as retry_error:
                        print(f"é‡è¯•æ’å…¥ä¹Ÿå¤±è´¥: {retry_error}")

            print(
                f"æ–‡ä»¶ {original_name} â†’ {storage_name} å¤„ç†å®Œæˆ: {success_count}/{len(content_chunks)} ä¸ªç‰‡æ®µå…¥åº“æˆåŠŸ")
            return success_count > 0

        except Exception as e:
            print(f"å­˜å‚¨æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
            return False

    def store_files(self, file_list: List[Dict]):
        """æ‰¹é‡å­˜å‚¨æ–‡ä»¶åˆ—è¡¨"""
        if not file_list:
            print("æ²¡æœ‰æ–‡ä»¶éœ€è¦å¤„ç†")
            return

        total_files = len(file_list)
        success_files = 0

        print(f"\n{'=' * 80}")
        print(f"å¼€å§‹å…¥åº“å¤„ç†ï¼Œå…± {total_files} ä¸ªæ–‡ä»¶")
        print(f"{'=' * 80}")

        for i, file_info in enumerate(file_list, 1):
            print(f"\n[{i}/{total_files}] å¤„ç†æ–‡ä»¶: {file_info['file_path']}")
            print(f"    å­˜å‚¨åç§°: {file_info['storage_name']}")

            if self.store_single_file(file_info):
                success_files += 1

            time.sleep(1)  # é¿å… API é™æµ

        # å°†æ•°æ®æŒä¹…åŒ–
        self.collection.flush()

        print(f"\n{'=' * 80}")
        print(f"å…¥åº“å¤„ç†å®Œæˆ!")
        print(f"æˆåŠŸ: {success_files}/{total_files} ä¸ªæ–‡ä»¶")
        print(f"å¤±è´¥: {total_files - success_files} ä¸ªæ–‡ä»¶")
        print(f"{'=' * 80}")

    def search_similar_papers(self, query, top_k=5):
        """æœç´¢ç›¸ä¼¼è®ºæ–‡"""
        try:
            # ç¡®ä¿é›†åˆå·²åŠ è½½
            self.collection.load()

            # è·å–æŸ¥è¯¢å‘é‡
            query_vector = self.get_embedding_with_qwen(query)

            # æœç´¢å‚æ•°
            search_params = {"metric_type": "L2", "params": {"nprobe": 10}}

            # æ‰§è¡Œæœç´¢
            results = self.collection.search(
                data=[query_vector],
                anns_field="vector",
                param=search_params,
                limit=top_k,
                output_fields=["file_name", "file_type", "directory", "content"]
            )

            print(f"\næœç´¢æŸ¥è¯¢: '{query}'")
            print(f"æ‰¾åˆ° {len(results[0])} ä¸ªç›¸å…³ç»“æœ:\n")

            for i, hit in enumerate(results[0]):
                content = hit.entity.get('content', '')
                content_preview = content[:200] + "..." if len(content) > 200 else content
                print(f"{i + 1}. æ–‡ä»¶: {hit.entity.get('file_name')}")
                print(f"   ç±»å‹: {hit.entity.get('file_type')}")
                print(f"   ç›®å½•: {hit.entity.get('directory')}")
                print(f"   è·ç¦»: {hit.distance:.4f}")
                print(f"   å†…å®¹é¢„è§ˆ: {content_preview}")
                print()

        except Exception as e:
            print(f"æœç´¢å¤±è´¥: {e}")

def insert_milvus_paper():
    # åˆå§‹åŒ–å­˜å‚¨ç±»
    store = MilvusPaperStore(
        host='localhost',
        port='19530',
        qwen_api_key=QWEN_API_KEY
    )

    # 1. é¦–å…ˆè·å–æ–‡ä»¶åˆ—è¡¨
    print("ç¬¬ä¸€æ­¥: æ‰«ææ–‡ä»¶...")
    file_list = store.get_target_files('./output_extracted')
    store.display_file_list(file_list)

    # 2. æµ‹è¯• API è¿æ¥
    print("\nç¬¬äºŒæ­¥: æµ‹è¯• API è¿æ¥...")
    test_vector = store.get_embedding_with_qwen("æµ‹è¯•æ–‡æœ¬")
    print(f"å‘é‡ç»´åº¦: {len(test_vector)}")

    # 3. å…¥åº“å¤„ç†
    print("\nç¬¬ä¸‰æ­¥: å¼€å§‹å…¥åº“...")
    store.store_files(file_list)

    # 4. æœç´¢æµ‹è¯•
    print("\nç¬¬å››æ­¥: æœç´¢æµ‹è¯•...")
    store.search_similar_papers("ç»ç’ƒå½¢æˆèƒ½åŠ› å•åŸå­é‡‘å±", top_k=3)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    insert_milvus_paper()